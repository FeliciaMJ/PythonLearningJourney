- 集成学习时通过结合几个模型的元算法，使得最后表现比任何一个模型都好。

- 偏差：由所有采样得到的大小为m的训练数据集，训练出所有模型的输出的平均值和真实模型输出之间的偏差。
  偏差通常是由学习算法的错误假设导致的。描述模型输出结果的期望与样本真实结果的差距。分类器表达能力有限导致的系统性错误，表现在训练误差不收敛。
- 方差：由所有采样得到的大小为m的训练数据集，训练出所有模型的输出的方差。
描述模型对给定值的输出稳定性。
- 偏差、方差和过拟合、欠拟合的关系：
一般来说，简单的模型会有一个较大的偏差和较小的方差，复杂的模型偏差较小而方法较大。
  > 欠拟合：模型不能适配训练样本，有一个很大的偏差。
  > 
  > 过拟合：模型很好的适配训练样本，但在测试集熵表现很糟，有一个很大的方差。
  
>   bagging: 减少 variance
> - bagging对于数据集进行取样，每个数据点有同等几率被采样，然后创建n个模型，每个模型进行m个数据采样，最后进行投票（voting）得出最后结果。
> 
> - 对训练样本进行采样，产生若干个不同的子集，再从子集中训练出一个分类器，取这些分类器的平均，所以是降低模型**方差**。
    

>   boosting: 减少 bias
> - Bagging的思想比较简单，它是均匀概率分布，有放回的，每个bag或者说predictor是平行的。
但boosting的每一次抽样的样本分布都是不一样的，每个predictor是有顺序的。
> 
> - 迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行权重调整，所以随着迭代的不断进行，误差会越来越小，所以模型的**偏差**会不断降低。


>   stacking: 增强预测效果
> - 其实和和bagging不同的是，stacking通常是不同的模型。