## 第5章 决策树

>  决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。
>  它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。
>  学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。
>  预测时，对新的数据，利用决策树模型进行分类。
>  决策树学习包括三个步骤：
>   - 特征选择
>   - 决策树的生成
>   - 决策树的修剪
---

### 决策树模型与学习
- 决策树由结点和有向边组成，结点由两种类型：内部结点和叶结点。
- 内部结点表示一个特征或属性，叶结点表示一个类。
- if-then的规则过程是这样的：从决策树的根结点到叶结点的每一条路径构建一条规则：路径上内部结点的特征对应着规则的条件，而叶结点
的类对应着规则的结论。
  
- 决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。
- 决策树的生成对应于模型的局部选择，决策树的剪纸对应于模型的全局选择。
- 决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。
- 决策树学习常用的算法有ID3,C4.5,CART。

---
### 特征选择
- 特征选择在于选取对训练数据具有分类能力的特征。通常选择特征的准则是信息增益或信息增益比。
- 熵：表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大。随机变量的概率分布-->熵
- 条件熵：表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量(X,Y)的联合概率分布--> 条件熵H(Y|X)
- 当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。
- 信息增益：特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。
信息增益大的特征具有更强的分类能力。
- 互信息：熵H(Y)与条件熵H(Y|X)之差。
- 信息增益值得大小是相对训练数据集而言的，没有绝对意义。
在分类问题困难时，也就是说训练数据集的经验熵大的时候，信息增益值会偏大，反之，信息增益值会偏小。
  
- 信息增益比：特征A对训练数据集D的信息增益比定义为信息增益g(D,A)与训练数据集D的经验熵H(D)之比。

---
### 决策树生成
- ID3算法
    - 核心是在决策树各个结点上应用**信息增益**准则选择特征，递归地构建决策树。
      ID3相当于用极大似然法进行概率模型的选择。
      ID3算法只有树的生成，所以该算法生成的树容易产生**过拟合**。
    - 具体方法：
      1. 从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值构建子结点。
      2. 再对子结点递归地调用以上方法，构建决策树。
      3. 直到所有特征的信息增益均很小或没有特征可以选择为止。
      4. 最后得到一个决策树。
    
- C4.5算法
    - C4.5在生成的过程中，用**信息增益比**来选择特征。
---

### 决策树的剪枝
- 剪枝：在决策树学习中将已生成的树进行简化的过程称为剪枝。
  剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的也结点，从而简化分类树模型。
  决策树的剪枝往往通过最小化决策树整体的损失函数或代价函数来实现。
  决策树的生成只考虑了通过提高信息增益(或信息增益比)对训练数据进行更好的拟合。
  而决策树的剪枝通过优化损失函数还考虑了减小模型负责度。
  
- 损失函数：求和(叶结点t的经验熵*叶结点t的样本点个数) + 叶结点的个数。
一组叶结点回缩到其父结点之前与之后的整体树分别为T<sub>B</sub>与T<sub>A</sub>,对应的损失函数值分别为
  C<sub>a</sub>(T<sub>A</sub>)与C<sub>a</sub>(T<sub>B</sub>),如果：
  C<sub>a</sub>(T<sub>A</sub>)<=C<sub>a</sub>(T<sub>B</sub>)则进行剪枝，将其父结点变为新的叶结点。
  
  
- 树的剪枝算法：
    - 计算每个结点的经验熵
    - 递归地从树的叶结点向上回缩
    
---
### CART算法
- CART同样由特征选择、树的生成和剪枝组成，既可以用于分类也可以用于回归。
CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的方法。
  
CART假设决策树是**二叉树**，内部结点特征的取值为"是"和"否"，
左分支是取值为"是"的分支，右分支是取值为"否"的分支。
这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，
也就是在输入给定的条件下输出的条件概率分布。

CART算法由以下两步组成：
> - 决策树的生成
>   - 决策树的生成就是递归地构建二叉决策树的过程。基于训练集生成决策树，生成的决策树要尽量大。
> 对**回归树**用**平方误差**最小化准则，对**分类树**采用**基尼系数**最小化准则，进行特征选择，生成二叉树。
> - 决策树的剪枝
>   - 用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。
---
































